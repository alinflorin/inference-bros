name: Load testing LLM
run-name: Load-testing - ${{ github.event.inputs.location }} - ${{ github.event.inputs.model }} - ${{ github.run_id }}

on:
  workflow_dispatch:
    inputs:
      location:
        description: "Location for Bifrost"
        required: true
        type: choice
        options:
          - stalpeni
      concurrency:
        description: "Concurrency"
        required: false
        default: "2"
      requestCount:
        description: "Request Count"
        required: false
        default: "8"
      model:
        description: "LLM model name"
        required: false
        default: "kubeai/qwen25-05b-instruct"
      tokenizer:
        description: "LLM model tokenizer"
        required: false
        default: "Qwen/Qwen2.5-0.5B-Instruct"
env:
  PYTHON_VERSION: '3.12'
  AIPERF_VERSION: '0.5.0'

jobs:
  load-test:
    runs-on: ubuntu-latest
    name: Load testing
    environment: ${{ github.event.inputs.location }}

    steps:
      - uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Cache Global Pip Tools
        uses: actions/cache@v5
        id: cache-global-tools
        with:
          path: ~/.local
          key: ${{ runner.os }}-global-aiperf-${{ env.AIPERF_VERSION }}

      - name: Cache Tokenizer
        uses: actions/cache@v5
        id: cache-tokenizer
        with:
          path: ~/.cache/huggingface/hub
          key: ${{ runner.os }}-tokenizer-${{ github.event.inputs.tokenizer }}
          restore-keys: |
            ${{ runner.os }}-tokenizer-

      - name: Add local bin to PATH
        run: echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install aiperf
        if: steps.cache-global-tools.outputs.cache-hit != 'true'
        run: pip install --user aiperf==${{ env.AIPERF_VERSION }}

      - name: Hydrate inputs
        run: |
          cat > prompts.jsonl << 'EOF'
          {"text": "Write a 300-word short story about a time-traveling toaster in the style of Sherlock Holmes."}
          {"text": "Summarize the pros and cons of remote work for a CEO who only has 30 seconds to read."}
          {"text": "If I have three apples and you take away two, how many apples do you have?"}
          {"text": "Translate the following sentence into French, Spanish, and German: 'The early bird catches the worm.'"}
          {"text": "Act as a grumpy 1920s detective and interrogate me about a missing sandwich."}
          {"text": "Explain the concept of 'opportunity cost' to a 10-year-old using a candy store analogy."}
          {"text": "Write a polite but firm email to a neighbor who keeps parking in my driveway."}
          {"text": "What are the three most important historical events of the 20th century? Justify your choices."}
          {"text": "Create a 5-day meal plan for a vegan who is allergic to nuts and hates kale."}
          {"text": "Solve this: A man is looking at a photograph. His friend asks who it is. The man replies, 'Brothers and sisters, I have none. But that man's father is my father's son.' Who is in the photograph?"}
          {"text": "Draft a product description for a 'smart umbrella' that never gets lost and predicts rain."}
          {"text": "Explain the plot of Inception using only emojis."}
          EOF

      - name: Run benchmark
        run: aiperf profile --input-file prompts.jsonl --custom-dataset-type single_turn --no-server-metrics --tokenizer ${{ github.event.inputs.tokenizer }} --ui-type none --api-key ${{ secrets.BIFROST_API_KEY }} --endpoint-type chat --endpoint /v1/chat/completions --request-count ${{ github.event.inputs.requestCount }} --concurrency ${{ github.event.inputs.concurrency }} --streaming --model ${{ github.event.inputs.model }} --url https://bifrost.${{ github.event.inputs.location }}.inferencebros.com

      - name: Upload Artifacts
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: aiperf-results-${{ github.run_id }}
          path: artifacts/

      - name: Generate Benchmark Summary
        if: always()
        shell: bash
        run: |
          python3 - <<EOF
          import csv
          import glob
          import os

          # Find the CSV regardless of the dynamic folder name
          files = glob.glob("artifacts/**/profile_export_aiperf.csv", recursive=True)
          if not files:
              print("No benchmark CSV found.")
              exit(1)

          csv_file = files[0]
          metrics = {}

          with open(csv_file, mode='r') as f:
              reader = csv.reader(f)
              for row in reader:
                  if not row or len(row) < 2: continue
                  # Handle both summary (Metric, Value) and distribution (Metric, avg, min...) rows
                  name = row[0].strip()
                  metrics[name] = row

          # Helper to extract a value from the metric row
          # For Summary items, index 1 is Value. For Distribution items, index 1 is Avg, index 11 is P99.
          def get_val(name, index=1):
              row = metrics.get(name)
              return row[index] if row and len(row) > index else "N/A"

          with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as s:
              s.write(f"## üöÄ LLM Load Test Results\n")
              s.write(f"**Model:** \`${{ github.event.inputs.model }}\` | **Concurrency:** \`${{ github.event.inputs.concurrency }}\`\n\n")
              
              s.write("### ‚è±Ô∏è Latency (User Experience)\n")
              s.write("| Metric | Average | P99 (Worst Case) |\n")
              s.write("| :--- | :--- | :--- |\n")
              s.write(f"| **Time to First Token (TTFT)** | {get_val('Time to First Token (ms)', 1)} ms | {get_val('Time to First Token (ms)', 11)} ms |\n")
              s.write(f"| **Inter-Token Latency (ITL)** | {get_val('Inter Token Latency (ms)', 1)} ms | {get_val('Inter Token Latency (ms)', 11)} ms |\n")
              s.write(f"| **Total Request Latency** | {get_val('Request Latency (ms)', 1)} ms | {get_val('Request Latency (ms)', 11)} ms |\n\n")

              s.write("### üìà Throughput & Efficiency\n")
              s.write("| Metric | Value |\n")
              s.write("| :--- | :--- |\n")
              s.write(f"| **Tokens / Sec (per user)** | {get_val('Output Token Throughput Per User (tokens/sec/user)')} |\n")
              s.write(f"| **Total System Throughput** | {get_val('Total Token Throughput (tokens/sec)')} tokens/s |\n")
              s.write(f"| **Requests / Sec** | {get_val('Request Throughput (requests/sec)')} |\n")
              s.write(f"| **Prefill Speed** | {get_val('Prefill Throughput Per User (tokens/sec/user)')} tokens/s |\n\n")

              s.write("### üõ†Ô∏è Technical Health\n")
              s.write(f"* **Total Requests:** {get_val('Request Count')} successful\n")
              s.write(f"* **Tokenizer Discrepancies:** {get_val('Usage Discrepancy Count')} (Should be 0)\n")
              s.write(f"* **Avg. Connection Overhead:** {get_val('HTTP Connection Overhead (ms)', 1)} ms\n")
          EOF