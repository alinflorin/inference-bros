name: Load testing LLM
run-name: Load-testing - ${{ github.event.inputs.location }} - ${{ github.event.inputs.model }} - ${{ github.run_id }}

on:
  workflow_dispatch:
    inputs:
      location:
        description: "Location for Bifrost"
        required: true
        type: choice
        options:
          - stalpeni
      apiKey:
        description: "API key for Bifrost"
        required: false
        default: ""
      concurrency:
        description: "Concurrency"
        required: false
        default: "10"
      requestCount:
        description: "Request Count"
        required: false
        default: "100"
      model:
        description: "LLM model name"
        required: false
        default: "kubeai/qwen25-05b-instruct"
      tokenizer:
        description: "LLM model tokenizer"
        required: false
        default: "Qwen/Qwen2.5-0.5B-Instruct"
env:
  PYTHON_VERSION: '3.12'
  AIPERF_VERSION: '0.5.0'

jobs:
  load-test:
    runs-on: ubuntu-latest
    name: Load testing

    steps:
      - uses: actions/setup-python@v6
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          
      - name: Cache Global Pip Tools
        uses: actions/cache@v5
        id: cache-global-tools
        with:
          path: ~/.local
          key: ${{ runner.os }}-global-aiperf-${{ env.AIPERF_VERSION }}

      - name: Cache Tokenizer
        uses: actions/cache@v5
        id: cache-tokenizer
        with:
          path: ~/.cache/huggingface/hub
          key: ${{ runner.os }}-tokenizer-${{ github.event.inputs.tokenizer }}
          restore-keys: |
            ${{ runner.os }}-tokenizer-

      - name: Add local bin to PATH
        run: echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install aiperf
        if: steps.cache-global-tools.outputs.cache-hit != 'true'
        run: pip install --user aiperf==${{ env.AIPERF_VERSION }}

      - name: Run benchmark
        run: aiperf profile --no-server-metrics --tokenizer ${{ github.event.inputs.tokenizer }} --ui-type none --api-key ${{ github.event.inputs.apiKey }} --endpoint-type chat --endpoint /v1/chat/completions --request-count ${{ github.event.inputs.requestCount }} --concurrency ${{ github.event.inputs.concurrency }} --streaming --model ${{ github.event.inputs.model }} --url https://bifrost.${{ github.event.inputs.location }}.inferencebros.com

      - name: Generate Summary from JSON
        if: always()
        run: |
          # Find the specific JSON file in the artifacts directory
          # The path includes model name and concurrency as you've seen
          JSON_PATH=$(find artifacts -name "profile_export_aiperf.json" | head -n 1)
          
          if [ -f "$JSON_PATH" ]; then
            echo "## ðŸ“Š NVIDIA AIPerf | LLM Metrics" >> $GITHUB_STEP_SUMMARY
            echo "Model: \`${{ github.event.inputs.model }}\` | Concurrency: \`${{ github.event.inputs.concurrency }}\`" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Create a Markdown Table using jq to parse the JSON metrics
            echo "| Metric | Avg | Min | Max | p99 | p50 |" >> $GITHUB_STEP_SUMMARY
            echo "| :--- | :--- | :--- | :--- | :--- | :--- |" >> $GITHUB_STEP_SUMMARY
            
            # This jq command maps the JSON keys to a readable table
            jq -r '.metrics | to_entries[] | "| \(.key) | \(.value.avg // "N/A") | \(.value.min // "N/A") | \(.value.max // "N/A") | \(.value.p99 // "N/A") | \(.value.p50 // "N/A") |"' "$JSON_PATH" >> $GITHUB_STEP_SUMMARY
          else
            echo "âŒ Error: profile_export_aiperf.json not found." >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload Artifacts
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: aiperf-results-${{ github.run_id }}
          path: artifacts/