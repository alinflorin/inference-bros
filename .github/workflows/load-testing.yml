name: Load testing LLM
run-name: Load-testing - ${{ github.event.inputs.LOCATION }} - ${{ github.event.inputs.MODEL_NAME }} - ${{ github.run_id }}

on:
  workflow_dispatch:
    inputs:
      LOCATION:
        description: "Location for Bifrost"
        required: true
        type: choice
        options:
          - stalpeni
      API_KEY:
        description: "API key for Bifrost"
        required: false
        default: ""
      MODEL_NAME:
        description: "LLM model name"
        required: false
        default: "qwen25-05b-instruct"
      MAX_CONCURRENCY:
        description: "Max concurrency"
        required: false
        default: "16"
      MAX_TOKENS:
        description: "Maximum tokens per request"
        required: false
        default: "512"
      NUM_WORDS:
        description: "Number of words in prompt"
        required: false
        default: "50"

env:
  BIN_VERSION: 'v1.0.7'

jobs:
  load-test:
    runs-on: ubuntu-latest
    name: Load testing

    steps:
      - name: Checkout repository
        uses: actions/checkout@v6

      - name: Set up cache for llmapibenchmark
        id: cache-binary
        uses: actions/cache@v5
        with:
          path: src/perf/llmapibenchmark_linux_amd64
          key: llmapibenchmark-${{ runner.os }}-${{ env.BIN_VERSION }}
          restore-keys: |
            llmapibenchmark-${{ runner.os }}-

      - name: Run load test
        working-directory: src/perf
        env:
          BIN_VERSION: ${{ env.BIN_VERSION }}
          LOCATION: ${{ github.event.inputs.LOCATION }}
          API_KEY: ${{ github.event.inputs.API_KEY }}
          MODEL_NAME: ${{ github.event.inputs.MODEL_NAME }}
          MAX_CONCURRENCY: ${{ github.event.inputs.MAX_CONCURRENCY }}
          MAX_TOKENS: ${{ github.event.inputs.MAX_TOKENS }}
          NUM_WORDS: ${{ github.event.inputs.NUM_WORDS }}
        run: |
          chmod +x ../perf/test.sh
          ../perf/test.sh

      - name: Generate summary
        if: always()
        working-directory: src/perf
        run: |
          echo "# ðŸš€ LLM Load Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          echo "## ðŸ“‹ Test Configuration" >> $GITHUB_STEP_SUMMARY
          echo "| Parameter | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|-----------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Location | \`${{ github.event.inputs.LOCATION }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Model | \`${{ github.event.inputs.MODEL_NAME }}\` |" >> $GITHUB_STEP_SUMMARY
          echo "| Max Concurrency | ${{ github.event.inputs.MAX_CONCURRENCY }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Max Tokens | ${{ github.event.inputs.MAX_TOKENS }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Prompt Words | ${{ github.event.inputs.NUM_WORDS }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ -f results.json ]; then
            # Extract overall metrics
            MODEL=$(jq -r '.model_name' results.json)
            INPUT_TOKENS=$(jq -r '.input_tokens' results.json)
            OUTPUT_TOKENS=$(jq -r '.output_tokens' results.json)
            LATENCY=$(jq -r '.latency' results.json)
            
            echo "## ðŸ“Š Overall Metrics" >> $GITHUB_STEP_SUMMARY
            echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
            echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
            echo "| Model | \`$MODEL\` |" >> $GITHUB_STEP_SUMMARY
            echo "| Input Tokens | $INPUT_TOKENS |" >> $GITHUB_STEP_SUMMARY
            echo "| Output Tokens | $OUTPUT_TOKENS |" >> $GITHUB_STEP_SUMMARY
            echo "| Latency (ms) | $LATENCY |" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Create performance table
            echo "## âš¡ Performance by Concurrency" >> $GITHUB_STEP_SUMMARY
            echo "| Concurrency | Generation Speed (tok/s) | Prompt Throughput (tok/s) | Min TTFT (s) | Max TTFT (s) | Success Rate |" >> $GITHUB_STEP_SUMMARY
            echo "|-------------|---------------------------|---------------------------|--------------|--------------|--------------|" >> $GITHUB_STEP_SUMMARY
            
            jq -r '.results[] | 
              "| \(.concurrency) | \(.generation_speed) | \(.prompt_throughput) | \(.min_ttft) | \(.max_ttft) | \(.success_rate * 100)% |"' \
              results.json >> $GITHUB_STEP_SUMMARY
            
            echo "" >> $GITHUB_STEP_SUMMARY
            
            # Add raw JSON in a collapsible section
            echo "<details>" >> $GITHUB_STEP_SUMMARY
            echo "<summary>ðŸ“„ Raw JSON Results</summary>" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo '```json' >> $GITHUB_STEP_SUMMARY
            jq '.' results.json >> $GITHUB_STEP_SUMMARY
            echo '```' >> $GITHUB_STEP_SUMMARY
            echo "</details>" >> $GITHUB_STEP_SUMMARY
          else
            echo "âš ï¸ **No results file found**" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Upload results artifact
        uses: actions/upload-artifact@v6
        if: always()
        with:
          name: llm-load-test-results
          path: src/perf/results.json
