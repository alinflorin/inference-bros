name: Load testing LLM
run-name: Load-testing - ${{ github.event.inputs.location }} - ${{ github.event.inputs.model }} - ${{ github.run_id }}

on:
  workflow_dispatch:
    inputs:
      location:
        description: "Location for Bifrost"
        required: true
        type: choice
        options:
          - stalpeni
      apiKey:
        description: "API key for Bifrost"
        required: false
        default: ""
      concurrency:
        description: "Concurrency"
        required: false
        default: "10"
      requestCount:
        description: "Request Count"
        required: false
        default: "100"
      model:
        description: "LLM model name"
        required: false
        default: "kubeai/qwen25-05b-instruct"

env:
  PYTHON_VERSION: '3.14'
  AIPERF_VERSION: '0.5.0'

jobs:
  load-test:
    runs-on: ubuntu-latest
    name: Load testing

    steps:
      - uses: actions/setup-python@v6
        with:
          python-version: '3.12'
          
      - name: Cache Pip Tools
        uses: actions/cache@v5
        id: cache-aiperf
        with:
          path: |
            ~/.local/bin
            ~/.local/lib/python${{ env.PYTHON_VERSION }}/site-packages
          key: ${{ runner.os }}-aiperf-${{ env.AIPERF_VERSION }}-py${{ env.PYTHON_VERSION }}

      - name: Add local bin to PATH
        run: echo "$HOME/.local/bin" >> $GITHUB_PATH

      - name: Install aiperf
        if: steps.cache-aiperf.outputs.cache-hit != 'true'
        run: pip install --user aiperf==${{ env.AIPERF_VERSION }}

      - name: Run benchmark
        run: aiperf profile --ui-type none --api-key ${{ github.event.inputs.apiKey }} --endpoint-type chat --endpoint /v1/chat/completions --request-count ${{ github.event.inputs.requestCount }} --concurrency ${{ github.event.inputs.concurrency }} --streaming --model ${{ github.event.inputs.model }} --url https://bifrost.${{ github.event.inputs.location }}.inferencebros.com

        